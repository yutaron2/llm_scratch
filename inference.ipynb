{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbe511d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "class SimpleGPTPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embed_size, num_heads, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(embed_size, num_heads, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # ソースをエンコード\n",
    "        # batch_first=True なので (batch, seq, embed) のまま\n",
    "        src_embedded = self.embedding(src)\n",
    "        encoded = self.encoder(src_embedded)\n",
    "        \n",
    "        # ターゲットをデコード\n",
    "        tgt_embedded = self.embedding(tgt)\n",
    "        \n",
    "        # ★追加3: 因果マスク (batch_first なので tgt.size(1) = seq_len)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
    "        \n",
    "        decoded = self.decoder(tgt_embedded, encoded, tgt_mask=tgt_mask)\n",
    "        output = self.lm_head(decoded)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device)) == 1\n",
    "        mask = mask.transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cf74f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 120\n"
     ]
    }
   ],
   "source": [
    "# Load vocab dictionaries (MUST match training order!)\n",
    "with open('inputLearnText.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))  # SORTED for consistency with training!\n",
    "char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
    "id_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def text_to_ids(text):\n",
    "    return [char_to_id[ch] for ch in text]\n",
    "\n",
    "def ids_to_text(ids):\n",
    "    return ''.join([id_to_char[i] for i in ids])\n",
    "\n",
    "print(f\"Vocab size: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4934cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "# NOTE: You need to RETRAIN with the new model that has positional encoding!\n",
    "# Old model weights won't work because the architecture changed.\n",
    "model = SimpleGPTPredictor(vocab_size=len(chars), embed_size=32, num_heads=4)\n",
    "model.load_state_dict(torch.load(\"model_10.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5ed2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model: SimpleGPTPredictor, input_text, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Predict next character with temperature control.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for prediction\n",
    "        input_text: Input text string\n",
    "        temperature: Controls randomness. Higher = more random, Lower = more deterministic\n",
    "                    temperature=1.0 is neutral, >1.0 is more random, <1.0 is more focused\n",
    "    \"\"\"\n",
    "    input_ids = text_to_ids(input_text)\n",
    "    input_tensor = torch.tensor([input_ids], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, input_tensor)\n",
    "        last_char_probs = output[0, -1, :]\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        last_char_probs = last_char_probs / temperature\n",
    "        probs = torch.softmax(last_char_probs, dim=-1)\n",
    "\n",
    "        # Sample from the distribution (instead of always picking top-1)\n",
    "        if temperature > 0:\n",
    "            char_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        else:\n",
    "            # If temperature is 0, use greedy (deterministic)\n",
    "            char_id = torch.argmax(probs).item()\n",
    "            \n",
    "        predicted_char = id_to_char[char_id]\n",
    "\n",
    "        return predicted_char\n",
    "\n",
    "def generateSeq(model, text, max_length=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate sequence with temperature control.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use\n",
    "        text: Starting text\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        temperature: Controls randomness (default 1.0)\n",
    "    \"\"\"\n",
    "    generated = text\n",
    "    for _ in range(max_length):\n",
    "        nextSingleToken = test_prediction(model, generated, temperature=temperature)\n",
    "        generated += nextSingleToken\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9017090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Temperature: 0.5 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substance by thean theresexte athere th\n",
      "\n",
      "=== Temperature: 0.8 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substanced prere dere riled theretherer\n",
      "\n",
      "=== Temperature: 1.0 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substance ta wasun bat tibre withe thur\n",
      "\n",
      "=== Temperature: 1.2 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substancerestly vinspextinset whrty e p\n",
      "\n",
      "=== Temperature: 1.5 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substancertan., Sbualentat: Axbicy wbur\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Their spiritual substance\"\n",
    "\n",
    "# Try different temperatures\n",
    "temperatures = [0.5, 0.8, 1.0, 1.2, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    completion = generateSeq(model, prompt, max_length=30, temperature=temp)\n",
    "    print(f\"\\n=== Temperature: {temp} ===\")\n",
    "    print(f\"入力: {prompt}\")\n",
    "    print(f\"出力: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d24af",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "### Key Changes:\n",
    "1. **batch_first=True** - Transformer layers now use (batch, seq, embed) format\n",
    "2. **No transpose operations** - Simpler code, more efficient\n",
    "3. **Vocab Consistency** - Both training and inference use sorted() vocab for consistent token IDs\n",
    "4. **Correct Mask Dimensions** - Uses `tgt.size(1)` for batch-first format\n",
    "\n",
    "### Architecture:\n",
    "- Encoder-Decoder Transformer\n",
    "- Embedding size: 32\n",
    "- Attention heads: 4\n",
    "- Layers: 2 (encoder) + 2 (decoder)\n",
    "- Causal masking for autoregressive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254b7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
