{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aca86b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from /data/home/ayumu/dev/llm_scratch/model.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import importlib\n",
    "import model as model_module\n",
    "\n",
    "importlib.reload(model_module)\n",
    "from model import SimpleGPTPredictor, device\n",
    "\n",
    "print(f\"Using model from {model_module.__file__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cf74f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 120\n"
     ]
    }
   ],
   "source": [
    "# Load vocab dictionaries (MUST match training order from main.py!)\n",
    "with open('inputLearnText.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))  # SORTED for consistency with training!\n",
    "char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
    "id_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def text_to_ids(text):\n",
    "    return [char_to_id[ch] for ch in text]\n",
    "\n",
    "def ids_to_text(ids):\n",
    "    return ''.join([id_to_char[i] for i in ids])\n",
    "\n",
    "print(f\"Vocab size: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4934cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred NUM_LAYERS = 10\n",
      "Model loaded from model/model_11.pth on cuda\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparams (must match main.py training config)\n",
    "EMBED_SIZE = 32\n",
    "NUM_HEADS = 4\n",
    "MAX_LEN = 100\n",
    "\n",
    "# Pick the checkpoint to load\n",
    "MODEL_PATH = \"model/model_11.pth\"\n",
    "\n",
    "# Load weights first so we can infer number of layers\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device, weights_only=True)\n",
    "\n",
    "def _infer_num_layers(sd, prefix=\"encoder.layers.\"):\n",
    "    layer_ids = {int(k.split('.')[2]) for k in sd if k.startswith(prefix)}\n",
    "    return (max(layer_ids) + 1) if layer_ids else 0\n",
    "\n",
    "NUM_LAYERS = _infer_num_layers(state_dict)\n",
    "print(f\"Inferred NUM_LAYERS = {NUM_LAYERS}\")\n",
    "\n",
    "# Instantiate model with same architecture as training\n",
    "model = SimpleGPTPredictor(\n",
    "    vocab_size=len(chars),\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    max_len=MAX_LEN,\n",
    "    num_layers=NUM_LAYERS,\n",
    ")\n",
    "\n",
    "# Load weights (strict=False allows old checkpoints without \"pe\")\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "if missing:\n",
    "    print(f\"Missing keys: {missing}\")\n",
    "if unexpected:\n",
    "    print(f\"Unexpected keys: {unexpected}\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from {MODEL_PATH} on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5ed2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model: SimpleGPTPredictor, input_text, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Predict next character with temperature control.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for prediction\n",
    "        input_text: Input text string\n",
    "        temperature: Controls randomness. Higher = more random, Lower = more deterministic\n",
    "                    temperature=1.0 is neutral, >1.0 is more random, <1.0 is more focused\n",
    "    \"\"\"\n",
    "    input_ids = text_to_ids(input_text)\n",
    "    input_tensor = torch.tensor([input_ids], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, input_tensor)\n",
    "        last_char_probs = output[0, -1, :]\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        last_char_probs = last_char_probs / temperature\n",
    "        probs = torch.softmax(last_char_probs, dim=-1)\n",
    "\n",
    "        # Sample from the distribution (instead of always picking top-1)\n",
    "        if temperature > 0:\n",
    "            char_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        else:\n",
    "            # If temperature is 0, use greedy (deterministic)\n",
    "            char_id = torch.argmax(probs).item()\n",
    "            \n",
    "        predicted_char = id_to_char[char_id]\n",
    "\n",
    "        return predicted_char\n",
    "\n",
    "def generateSeq(model, text, max_length=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate sequence with temperature control.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use\n",
    "        text: Starting text\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        temperature: Controls randomness (default 1.0)\n",
    "    \"\"\"\n",
    "    generated = text\n",
    "    for _ in range(max_length):\n",
    "        nextSingleToken = test_prediction(model, generated, temperature=temperature)\n",
    "        generated += nextSingleToken\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9017090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Temperature: 0.5 ===\n",
      "Input:  But since what may prove \n",
      "Output: But since what may prove  e       i o  t d  d t  etto    e ee rn  et     l \n",
      "\n",
      "=== Temperature: 0.8 ===\n",
      "Input:  But since what may prove \n",
      "Output: But since what may prove yhy surns  e ih ,no a.au  smie.c;sfct o\n",
      " en ld loa\n",
      "\n",
      "=== Temperature: 1.0 ===\n",
      "Input:  But since what may prove \n",
      "Output: But since what may prove eatBamteh”em rph4eeaepοne;t2.eone ehifo;ioeibyAhhs\n",
      "\n",
      "=== Temperature: 1.2 ===\n",
      "Input:  But since what may prove \n",
      "Output: But since what may prove sdPrlc bllμ wwoe νgreᾶia7e iyngx) kat]έ\n",
      "pa7lott wa\n",
      "\n",
      "=== Temperature: 1.5 ===\n",
      "Input:  But since what may prove \n",
      "Output: But since what may prove wve)kώεōvח,αCtECκe kiuh”wuunsgc  oc ehJiSihlςόydat\n"
     ]
    }
   ],
   "source": [
    "prompt = \"But since what may prove \"\n",
    "\n",
    "# Try different temperatures\n",
    "temperatures = [0.5, 0.8, 1.0, 1.2, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    completion = generateSeq(model, prompt, max_length=50, temperature=temp)\n",
    "    print(f\"\\n=== Temperature: {temp} ===\")\n",
    "    print(f\"Input:  {prompt}\")\n",
    "    print(f\"Output: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d24af",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "### Key Changes:\n",
    "1. **batch_first=True** - Transformer layers now use (batch, seq, embed) format\n",
    "2. **No transpose operations** - Simpler code, more efficient\n",
    "3. **Vocab Consistency** - Both training and inference use sorted() vocab for consistent token IDs\n",
    "4. **Correct Mask Dimensions** - Uses `tgt.size(1)` for batch-first format\n",
    "\n",
    "### Architecture:\n",
    "- Encoder-Decoder Transformer\n",
    "- Embedding size: 32\n",
    "- Attention heads: 4\n",
    "- Layers: 2 (encoder) + 2 (decoder)\n",
    "- Causal masking for autoregressive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254b7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
