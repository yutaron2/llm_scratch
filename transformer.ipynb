{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-nSYqMhHZucx",
        "outputId": "5c534982-0907-4293-a005-5e308db68851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91jX2ItZhAUE"
      },
      "source": [
        "<h2>予測コード</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GgzK6kdNZ7Et",
        "outputId": "3f60c2dc-a2d4-4721-f955-16509c955870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "文字→ID辞書: {'-': 0, 'æ': 1, ' ': 2, 'έ': 3, 'm': 4, 'X': 5, 'α': 6, 'G': 7, 'ה': 8, 'ἐ': 9, 'R': 10, 'D': 11, 'V': 12, '[': 13, 'כ': 14, ':': 15, 'ῳ': 16, 'a': 17, 'c': 18, 'q': 19, 'p': 20, '6': 21, 'ν': 22, 'w': 23, 'ω': 24, 'ᾶ': 25, 'ή': 26, '”': 27, 'E': 28, 'L': 29, 'O': 30, '8': 31, '.': 32, '3': 33, 'e': 34, 'f': 35, 'U': 36, 'b': 37, 'j': 38, '5': 39, 'ώ': 40, 'מ': 41, 'ῃ': 42, 'Z': 43, '9': 44, '“': 45, 'ἀ': 46, 'P': 47, 'y': 48, 'τ': 49, 'σ': 50, 'μ': 51, 'W': 52, 'ρ': 53, '4': 54, 'i': 55, 'ο': 56, 'v': 57, 'ῆ': 58, 't': 59, 'ς': 60, 'ε': 61, 'κ': 62, 'A': 63, 'λ': 64, 'ὕ': 65, 'ָ': 66, 'n': 67, 'k': 68, 'י': 69, 'T': 70, 'M': 71, 'o': 72, 'ῥ': 73, 'υ': 74, 'x': 75, 'ἰ': 76, 'l': 77, 'Æ': 78, ';': 79, '(': 80, 'F': 81, 'u': 82, 'ό': 83, ']': 84, '\\n': 85, 'ι': 86, 'ו': 87, 'C': 88, '?': 89, 'χ': 90, 'I': 91, ',': 92, 'Y': 93, 'z': 94, '2': 95, 'H': 96, '1': 97, 'ō': 98, 'η': 99, 'B': 100, 'ְ': 101, 'N': 102, 'ύ': 103, 'ί': 104, '0': 105, ')': 106, 'd': 107, 'π': 108, 's': 109, 'ῤ': 110, '7': 111, 'r': 112, 'g': 113, 'S': 114, 'h': 115, 'ח': 116, '’': 117, '—': 118, 'J': 119}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleGPTPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # K, V, Qとかの計算->残渣結合->正規化->FFN (Feed Forward Network)とかをやってるぽい\n",
        "        # Attention計算では、QとKの（各トークンのKにたいする）内積を計算して、それをベクトルとしてもつ。そのベクトルをSoftmaxで重みにする。\n",
        "        # 各トークンのVと重みつき平均を取る。\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embed_size, num_heads),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        # Linearは、行列積 shapeは？\n",
        "        # \n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (sequence,)の行ベクトル\n",
        "        # [sequenceLength, embedSize]\n",
        "        embedded = self.embedding(x)\n",
        "        # 文脈情報を考慮してベクトルを更新（形状そのまま,[sequenceLength, embedSize]\n",
        "        transformed = self.transformer(embedded)\n",
        "        # 非正規化前のスコア[sequenceLength, vocabSize]\n",
        "        output = self.lm_head(transformed)\n",
        "        \n",
        "        return output\n",
        "\n",
        "with open('inputLearnText.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = list(set(text))  #hello->heloみたいにする。\n",
        "char_to_id = {ch: i for i, ch in enumerate(chars)} # 文字: idの配列\n",
        "id_to_char = {i: ch for i, ch in enumerate(chars)} # id: 文字の配列\n",
        "\n",
        "print(\"文字→ID辞書:\", char_to_id)\n",
        "\n",
        "# テキストを1文字ずつIDに変換\n",
        "def text_to_ids(text):\n",
        "    return [char_to_id[ch] for ch in text]\n",
        "\n",
        "# IDを1文字ずつテキストに変換\n",
        "def ids_to_text(ids):\n",
        "    return ''.join([id_to_char[i] for i in ids])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SimpleGPTPredictor(vocab_size=len(chars), embed_size=32, num_heads=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzHLLWKSg3pa"
      },
      "source": [
        "<h2>学習用コード</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_frHUYuSZxSk",
        "outputId": "0551958b-9248-4976-d11b-a0d519c4bd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "学習データ数: 51642\n",
            "例 - 入力: ' Fathers w'\n",
            "例 - 正解: 'Fathers wi'\n",
            "\n",
            "学習開始...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Placeholder storage has not been allocated on MPS device!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# 予測 この記法でforward()を実行できる。\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 出力は正規化前の値 例[2.1, -0.5, 3.2, 0.8, -1.1, ...]\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_inputs[i:i\u001b[38;5;241m+\u001b[39mbatchSize])\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# これもcall?\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# shapeを変更。\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# original_tensor = [\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#   [21,22,23,24]   # バッチ2-位置3\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m     86\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chars)), train_targets[i:i\u001b[38;5;241m+\u001b[39mbatchSize]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mSimpleGPTPredictor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# 埋め込み\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# 文脈情報を考慮してベクトルを更新（形状そのまま）\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(embedded)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/sparse.py:192\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[1;32m    200\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/functional.py:2542\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2537\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "mps_device = torch.device(\"mps\")\n",
        "# テキストを1文字ずつIDに変換\n",
        "def text_to_ids(text):\n",
        "    return [char_to_id[ch] for ch in text]\n",
        "\n",
        "# IDを1文字ずつテキストに変換\n",
        "def ids_to_text(ids):\n",
        "    return ''.join([id_to_char[i] for i in ids])\n",
        "\n",
        "# 簡単な学習データ作成\n",
        "def create_training_data(text, seq_len=10):\n",
        "    ids = text_to_ids(text)\n",
        "    inputs, targets = [], []\n",
        "\n",
        "    for i in range(len(ids) - seq_len):\n",
        "        inputs.append(ids[i:i+seq_len])        # 入力：10文字チャンク\n",
        "        targets.append(ids[i+1:seq_len+1])   # 正解：1文字ずらしたチャンク\n",
        "\n",
        "    return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "# 学習データ作成\n",
        "# 追加する\n",
        "with open('inputLearnText.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# これなんだっけ\n",
        "chars = set(text)\n",
        "\n",
        "char_to_id = {ch: i for i, ch in enumerate(chars)} # 文字: idの配列\n",
        "id_to_char = {i: ch for i, ch in enumerate(chars)} # id: 文字の配列\n",
        "\n",
        "train_inputs, train_targets = create_training_data(text)\n",
        "\n",
        "print(f\"学習データ数: {len(train_inputs)}\")\n",
        "print(f\"例 - 入力: '{ids_to_text(train_inputs[20].tolist())}'\")\n",
        "print(f\"例 - 正解: '{ids_to_text(train_targets[20].tolist())}'\")\n",
        "\n",
        "\n",
        "model = SimpleGPTPredictor(vocab_size=len(chars), embed_size=32, num_heads=4)\n",
        "# 学習設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# 損失関数の種類\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\n学習開始...\")\n",
        "# range\n",
        "train_inputs = train_inputs[:5000]\n",
        "train_targets = train_targets[:5000]\n",
        "\n",
        "for epoch in range(10000):\n",
        "    total_loss = 0\n",
        "    batchSize = 256\n",
        "    for i in range(0, len(train_inputs), batchSize):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 予測 この記法でforward()を実行できる。\n",
        "        # 出力は正規化前の値 例[2.1, -0.5, 3.2, 0.8, -1.1, ...]\n",
        "        output = model(train_inputs[i:i+batchSize])\n",
        "\n",
        "        # (batch, seq, vocab) ->  #(seq, )\n",
        "        # \n",
        "        # ------- before -------\n",
        "        # \n",
        "        # \n",
        "        # original_tensor = [\n",
        "        # [  # バッチ1\n",
        "        #     [1, 2, 3, 4],  # 位置1の予測ベクトル\n",
        "        #     [5, 6, 7, 8],  # 位置2の予測ベクトル  \n",
        "        #     [9,10,11,12]   # 位置3の予測ベクトル\n",
        "        # ],\n",
        "        # [  # バッチ2\n",
        "        #     [13,14,15,16], # 位置1の予測ベクトル\n",
        "        #     [17,18,19,20], # 位置2の予測ベクトル\n",
        "        #     [21,22,23,24]  # 位置3の予測ベクトル\n",
        "        # ]\n",
        "        # ]\n",
        "        #\n",
        "        # ------- after -------\n",
        "        #         reshaped_tensor = [\n",
        "        #   [1, 2, 3, 4],   # バッチ1-位置1\n",
        "        #   [5, 6, 7, 8],   # バッチ1-位置2  \n",
        "        #   [9,10,11,12],   # バッチ1-位置3\n",
        "        #   [13,14,15,16],  # バッチ2-位置1\n",
        "        #   [17,18,19,20],  # バッチ2-位置2\n",
        "        #   [21,22,23,24]   # バッチ2-位置3\n",
        "        # ]\n",
        "        # train_targetsは(batch*seq)の一次元。要するに次元を減らしている。（数値の変更とかはない）\n",
        "        loss = criterion(output.view(-1, len(chars)), train_targets[i:i+batchSize].view(-1))\n",
        "\n",
        "        # 学習\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss/len(train_inputs):.4f}\")\n",
        "\n",
        "print(\"学習完了！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZPiJSE7kcD1"
      },
      "source": [
        "<h2>予測テスト</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgy_HYznkb59",
        "outputId": "590ae9cd-dc3e-41a8-da05-6c1e8ab306eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "入力テキスト:  The god is a\n",
            "回答:  The god is aS7ggugÆGW)vπηrb“7gugu\n"
          ]
        }
      ],
      "source": [
        "def test_prediction(model: SimpleGPTPredictor, input_text):\n",
        "    input_ids = text_to_ids(input_text)\n",
        "    input_tensor = torch.tensor([input_ids])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        last_char_probs = output[0, -1, :]\n",
        "        probs = torch.softmax(last_char_probs, dim=-1)\n",
        "\n",
        "        # 一位のトークンを呼び出す。\n",
        "        top_prob, top_index = torch.topk(probs, 1)\n",
        "        char_id = top_index.item()  # テンソルから数値を取り出し\n",
        "        predicted_char = id_to_char[char_id]  # IDを文字に変換\n",
        "\n",
        "        return predicted_char\n",
        "\n",
        "def generateSeq(model, text, count = 0):\n",
        "    nextSingleToken = test_prediction(model, text)\n",
        "    if(count < 20):\n",
        "        return generateSeq(model, text+nextSingleToken, count+1)\n",
        "    else:\n",
        "        return text+nextSingleToken\n",
        "\n",
        "prompt = \"The god is a\"\n",
        "completion = generateSeq(model, prompt)\n",
        "print(\"入力テキスト: \", prompt)\n",
        "print(\"回答: \", completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5hdog7Xkbo8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki-ziXqGZxqu"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
