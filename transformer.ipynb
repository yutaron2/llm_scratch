{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-nSYqMhHZucx",
        "outputId": "5c534982-0907-4293-a005-5e308db68851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
            "python: can't open file '/Users/nishidayuutarou/dev/indv/ml/train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91jX2ItZhAUE"
      },
      "source": [
        "<h2>予測コード</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GgzK6kdNZ7Et",
        "outputId": "3f60c2dc-a2d4-4721-f955-16509c955870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "文字→ID辞書: {'”': 0, '0': 1, 'k': 2, 'ה': 3, 'μ': 4, 'ἀ': 5, 'y': 6, '-': 7, 'υ': 8, 'ε': 9, 'j': 10, 'χ': 11, 'm': 12, 'O': 13, 'E': 14, 'Y': 15, '“': 16, '.': 17, 'C': 18, ',': 19, 'W': 20, 'P': 21, 'M': 22, 'ύ': 23, 'F': 24, '9': 25, 'G': 26, ' ': 27, '7': 28, '\\n': 29, 'ְ': 30, 'h': 31, 'ι': 32, 'ח': 33, 'f': 34, 'v': 35, ']': 36, 'b': 37, 'י': 38, 'i': 39, 'מ': 40, '1': 41, 'g': 42, 'ρ': 43, ':': 44, 'p': 45, 'ἐ': 46, '’': 47, 'ו': 48, 'o': 49, 'ή': 50, 'η': 51, '?': 52, 'σ': 53, 'ν': 54, 'ῃ': 55, 'n': 56, 'e': 57, 'V': 58, 'U': 59, 'X': 60, 'J': 61, 'ῆ': 62, 'L': 63, '6': 64, '—': 65, 'ί': 66, 'α': 67, 'כ': 68, 'ς': 69, 'ῥ': 70, 'ῳ': 71, ')': 72, 'τ': 73, 'ο': 74, 'ὕ': 75, 'z': 76, '8': 77, 'S': 78, 'l': 79, 'w': 80, 'κ': 81, 'd': 82, 'H': 83, 'D': 84, '[': 85, 'Æ': 86, 'u': 87, '2': 88, '5': 89, 'π': 90, 'R': 91, 'x': 92, 'ō': 93, 'ῤ': 94, 'A': 95, 'B': 96, 'æ': 97, 'ό': 98, 'c': 99, 'ἰ': 100, 't': 101, 'T': 102, 'ָ': 103, '(': 104, 'ώ': 105, 'ᾶ': 106, 'q': 107, '4': 108, 'ω': 109, 's': 110, 'Z': 111, ';': 112, 'a': 113, 'N': 114, '3': 115, 'r': 116, 'έ': 117, 'I': 118, 'λ': 119}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleGPTPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # K, V, Qとかの計算->残渣結合->正規化->FFN (Feed Forward Network)とかをやってるぽい\n",
        "        # Attention計算では、QとKの（各トークンのKにたいする）内積を計算して、それをベクトルとしてもつ。そのベクトルをSoftmaxで重みにする。\n",
        "        # 各トークンのVと重みつき平均を取る。\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embed_size, num_heads),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 埋め込み\n",
        "        embedded = self.embedding(x)\n",
        "        # 文脈情報を考慮してベクトルを更新（形状そのまま）\n",
        "        transformed = self.transformer(embedded)\n",
        "        # 非正規化前のスコア\n",
        "        output = self.lm_head(transformed)\n",
        "        \n",
        "        return output\n",
        "\n",
        "with open('inputLearnText.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = list(set(text))  #hello->heloみたいにする。\n",
        "char_to_id = {ch: i for i, ch in enumerate(chars)} # 文字: idの配列\n",
        "id_to_char = {i: ch for i, ch in enumerate(chars)} # id: 文字の配列\n",
        "\n",
        "print(\"文字→ID辞書:\", char_to_id)\n",
        "\n",
        "# テキストを1文字ずつIDに変換\n",
        "def text_to_ids(text):\n",
        "    return [char_to_id[ch] for ch in text]\n",
        "\n",
        "# IDを1文字ずつテキストに変換\n",
        "def ids_to_text(ids):\n",
        "    return ''.join([id_to_char[i] for i in ids])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzHLLWKSg3pa"
      },
      "source": [
        "<h2>学習用コード</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_frHUYuSZxSk",
        "outputId": "0551958b-9248-4976-d11b-a0d519c4bd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "学習データ数: 51642\n",
            "例 - 入力: ' Fathers w'\n",
            "例 - 正解: 'Fathers wi'\n",
            "\n",
            "学習開始...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# テキストを1文字ずつIDに変換\n",
        "def text_to_ids(text):\n",
        "    return [char_to_id[ch] for ch in text]\n",
        "\n",
        "# IDを1文字ずつテキストに変換\n",
        "def ids_to_text(ids):\n",
        "    return ''.join([id_to_char[i] for i in ids])\n",
        "\n",
        "# 簡単な学習データ作成\n",
        "def create_training_data(text, seq_len=10):\n",
        "    ids = text_to_ids(text)\n",
        "    inputs, targets = [], []\n",
        "\n",
        "    for i in range(len(ids) - seq_len):\n",
        "        inputs.append(ids[i:i+seq_len])        # 入力：10文字\n",
        "        targets.append(ids[i+1:i+seq_len+1])   # 正解：1文字ずらし\n",
        "\n",
        "    return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "# 学習データ作成\n",
        "# 追加する\n",
        "with open('inputLearnText.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# これなんだっけ\n",
        "chars = set(text)\n",
        "\n",
        "char_to_id = {ch: i for i, ch in enumerate(chars)} # 文字: idの配列\n",
        "id_to_char = {i: ch for i, ch in enumerate(chars)} # id: 文字の配列\n",
        "\n",
        "train_inputs, train_targets = create_training_data(text)\n",
        "\n",
        "print(f\"学習データ数: {len(train_inputs)}\")\n",
        "print(f\"例 - 入力: '{ids_to_text(train_inputs[20].tolist())}'\")\n",
        "print(f\"例 - 正解: '{ids_to_text(train_targets[20].tolist())}'\")\n",
        "\n",
        "\n",
        "model = SimpleGPTPredictor(vocab_size=len(chars), embed_size=32, num_heads=4)\n",
        "# 学習設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# 損失関数の種類\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\n学習開始...\")\n",
        "# range\n",
        "train_inputs = train_inputs[:5000]\n",
        "train_targets = train_targets[:5000]\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    batchSize = 128\n",
        "    for i in range(0, len(train_inputs), batchSize):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 予測 この記法でforward()を実行できる。\n",
        "        # 出力は正規化前の値 例[2.1, -0.5, 3.2, 0.8, -1.1, ...]\n",
        "        output = model(train_inputs[i:i+batchSize])\n",
        "        # これもcall?\n",
        "        loss = criterion(output.view(-1, len(chars)), train_targets[i:i+batchSize].view(-1))\n",
        "\n",
        "        # 学習\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss/len(train_inputs):.4f}\")\n",
        "\n",
        "print(\"学習完了！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZPiJSE7kcD1"
      },
      "source": [
        "<h2>予測テスト</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgy_HYznkb59",
        "outputId": "590ae9cd-dc3e-41a8-da05-6c1e8ab306eb"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'お'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[61], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m+\u001b[39mnextSingleToken\n\u001b[1;32m     24\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mおはよう\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m completion \u001b[38;5;241m=\u001b[39m generateSeq(model, prompt)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m入力テキスト: \u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m回答: \u001b[39m\u001b[38;5;124m\"\u001b[39m, completion)\n",
            "Cell \u001b[0;32mIn[61], line 18\u001b[0m, in \u001b[0;36mgenerateSeq\u001b[0;34m(model, text, count)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerateSeq\u001b[39m(model, text, count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     nextSingleToken \u001b[38;5;241m=\u001b[39m test_prediction(model, text)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generateSeq(model, text\u001b[38;5;241m+\u001b[39mnextSingleToken, count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[0;32mIn[61], line 2\u001b[0m, in \u001b[0;36mtest_prediction\u001b[0;34m(model, input_text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_prediction\u001b[39m(model, input_text):\n\u001b[0;32m----> 2\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m text_to_ids(input_text)\n\u001b[1;32m      3\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([input_ids])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "Cell \u001b[0;32mIn[42], line 39\u001b[0m, in \u001b[0;36mtext_to_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtext_to_ids\u001b[39m(text):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [char_to_id[ch] \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m text]\n",
            "\u001b[0;31mKeyError\u001b[0m: 'お'"
          ]
        }
      ],
      "source": [
        "def test_prediction(model, input_text):\n",
        "    input_ids = text_to_ids(input_text)\n",
        "    input_tensor = torch.tensor([input_ids])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        last_char_probs = output[0, -1, :]\n",
        "        probs = torch.softmax(last_char_probs, dim=-1)\n",
        "\n",
        "        # 一位のトークンを呼び出す。\n",
        "        top_prob, top_index = torch.topk(probs, 1)\n",
        "        char_id = top_index.item()  # テンソルから数値を取り出し\n",
        "        predicted_char = id_to_char[char_id]  # IDを文字に変換\n",
        "\n",
        "        return predicted_char\n",
        "\n",
        "def generateSeq(model, text, count = 0):\n",
        "    nextSingleToken = test_prediction(model, text)\n",
        "    if(count < 20):\n",
        "        return generateSeq(model, text+nextSingleToken, count+1)\n",
        "    else:\n",
        "        return text+nextSingleToken\n",
        "    \n",
        "prompt = \"おはよう\"\n",
        "completion = generateSeq(model, prompt)\n",
        "print(\"入力テキスト: \", prompt)\n",
        "print(\"回答: \", completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5hdog7Xkbo8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki-ziXqGZxqu"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
