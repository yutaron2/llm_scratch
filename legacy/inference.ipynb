{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca86b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe511d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "class SimpleGPTPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # token数の上限\n",
    "        self.max_len = max_len\n",
    "        # このへんでpos encoding\n",
    "        self.pe = self.positional_encoding(max_len, embed_size) # (max_legnth, embed_size)\n",
    "\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embed_size, num_heads, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(embed_size, num_heads, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt): # (b, seq_length)\n",
    "        \"\"\"\n",
    "        pe = \n",
    "        [\n",
    "            [12, 123, 1, 4, 5 ,1 ,4],\n",
    "            [12, 123, 1, 4, 5 ,1 ,4]\n",
    "        ]\n",
    "        [\n",
    "            [13434, 2343,  3234,...],\n",
    "            [32432, 343324, 4343,...],\n",
    "            [],\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # scr（peは↑のような固定値の配列なので、入力エンべディングのサイズに合わせて切り取る）\n",
    "        src_p = self.pe[:src.size(1), :].to(src.device) # (seq, embed_dim)\n",
    "        target_p = self.pe[:src.size(1), :].to(tgt.device)\n",
    "\n",
    "        # ソースをエンコード\n",
    "        # batch_first=True なので (batch, seq, embed) のまま\n",
    "        src_embedded = self.embedding(src) + src_p\n",
    "        encoded = self.encoder(src_embedded) + target_p\n",
    "        z\n",
    "        # ターゲットをデコード\n",
    "        tgt_embedded = self.embedding(tgt)\n",
    "        \n",
    "        # ★追加3: 因果マスク (batch_first なので tgt.size(1) = seq_len)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
    "        \n",
    "        decoded = self.decoder(tgt_embedded, encoded, tgt_mask=tgt_mask)\n",
    "        output = self.lm_head(decoded)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device)) == 1\n",
    "        mask = mask.transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    import math\n",
    "\n",
    "    def positional_encoding(max_len, embed_size):\n",
    "        pe = torch.zeros(max_len, embed_size) # [max_length, embedd_size]\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, embed_size, 2):\n",
    "                pe[pos, i]     = math.sin(pos / (10000 ** (i / embed_size)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / embed_size)))\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf74f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 120\n"
     ]
    }
   ],
   "source": [
    "# Load vocab dictionaries (MUST match training order!)\n",
    "with open('inputLearnText.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))  # SORTED for consistency with training!\n",
    "char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
    "id_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def text_to_ids(text):\n",
    "    return [char_to_id[ch] for ch in text]\n",
    "\n",
    "def ids_to_text(ids):\n",
    "    return ''.join([id_to_char[i] for i in ids])\n",
    "\n",
    "print(f\"Vocab size: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4934cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "# NOTE: You need to RETRAIN with the new model that has positional encoding!\n",
    "# Old model weights won't work because the architecture changed.\n",
    "model = SimpleGPTPredictor(vocab_size=len(chars), embed_size=32, num_heads=4)\n",
    "model_config = \"model_35.pth\"\n",
    "\n",
    "if device == 'mps':\n",
    "  model.load_state_dict(torch.load(model_config, map_location=device))\n",
    "else:\n",
    "  model.load_state_dict(torch.load(model_config))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ed2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model: SimpleGPTPredictor, input_text, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Predict next character with temperature control.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for prediction\n",
    "        input_text: Input text string\n",
    "        temperature: Controls randomness. Higher = more random, Lower = more deterministic\n",
    "                    temperature=1.0 is neutral, >1.0 is more random, <1.0 is more focused\n",
    "    \"\"\"\n",
    "    input_ids = text_to_ids(input_text)\n",
    "    input_tensor = torch.tensor([input_ids], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, input_tensor)\n",
    "        last_char_probs = output[0, -1, :]\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        last_char_probs = last_char_probs / temperature\n",
    "        probs = torch.softmax(last_char_probs, dim=-1)\n",
    "\n",
    "        # Sample from the distribution (instead of always picking top-1)\n",
    "        if temperature > 0:\n",
    "            char_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        else:\n",
    "            # If temperature is 0, use greedy (deterministic)\n",
    "            char_id = torch.argmax(probs).item()\n",
    "            \n",
    "        predicted_char = id_to_char[char_id]\n",
    "\n",
    "        return predicted_char\n",
    "\n",
    "def generateSeq(model, text, max_length=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate sequence with temperature control.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use\n",
    "        text: Starting text\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        temperature: Controls randomness (default 1.0)\n",
    "    \"\"\"\n",
    "    generated = text\n",
    "    for _ in range(max_length):\n",
    "        nextSingleToken = test_prediction(model, generated, temperature=temperature)\n",
    "        generated += nextSingleToken\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9017090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Temperature: 0.5 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substance te thert thereinte the the th\n",
      "\n",
      "=== Temperature: 0.8 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substancerible anche witere then, te th\n",
      "\n",
      "=== Temperature: 1.0 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substance t incernleserriteritererincri\n",
      "\n",
      "=== Temperature: 1.2 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substancedited teding the prospo dupthe\n",
      "\n",
      "=== Temperature: 1.5 ===\n",
      "入力: Their spiritual substance\n",
      "出力: Their spiritual substance rthe \n",
      " Hispaprim, Hens, can a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Their spiritual substance\"\n",
    "\n",
    "# Try different temperatures\n",
    "temperatures = [0.5, 0.8, 1.0, 1.2, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    completion = generateSeq(model, prompt, max_length=30, temperature=temp)\n",
    "    print(f\"\\n=== Temperature: {temp} ===\")\n",
    "    print(f\"入力: {prompt}\")\n",
    "    print(f\"出力: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d24af",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "### Key Changes:\n",
    "1. **batch_first=True** - Transformer layers now use (batch, seq, embed) format\n",
    "2. **No transpose operations** - Simpler code, more efficient\n",
    "3. **Vocab Consistency** - Both training and inference use sorted() vocab for consistent token IDs\n",
    "4. **Correct Mask Dimensions** - Uses `tgt.size(1)` for batch-first format\n",
    "\n",
    "### Architecture:\n",
    "- Encoder-Decoder Transformer\n",
    "- Embedding size: 32\n",
    "- Attention heads: 4\n",
    "- Layers: 2 (encoder) + 2 (decoder)\n",
    "- Causal masking for autoregressive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254b7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
