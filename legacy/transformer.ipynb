{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-nSYqMhHZucx",
        "outputId": "5c534982-0907-4293-a005-5e308db68851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91jX2ItZhAUE"
      },
      "source": [
        "<h2>予測コード</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GgzK6kdNZ7Et",
        "outputId": "3f60c2dc-a2d4-4721-f955-16509c955870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "文字→ID辞書: {'-': 0, 'æ': 1, ' ': 2, 'έ': 3, 'm': 4, 'X': 5, 'α': 6, 'G': 7, 'ה': 8, 'ἐ': 9, 'R': 10, 'D': 11, 'V': 12, '[': 13, 'כ': 14, ':': 15, 'ῳ': 16, 'a': 17, 'c': 18, 'q': 19, 'p': 20, '6': 21, 'ν': 22, 'w': 23, 'ω': 24, 'ᾶ': 25, 'ή': 26, '”': 27, 'E': 28, 'L': 29, 'O': 30, '8': 31, '.': 32, '3': 33, 'e': 34, 'f': 35, 'U': 36, 'b': 37, 'j': 38, '5': 39, 'ώ': 40, 'מ': 41, 'ῃ': 42, 'Z': 43, '9': 44, '“': 45, 'ἀ': 46, 'P': 47, 'y': 48, 'τ': 49, 'σ': 50, 'μ': 51, 'W': 52, 'ρ': 53, '4': 54, 'i': 55, 'ο': 56, 'v': 57, 'ῆ': 58, 't': 59, 'ς': 60, 'ε': 61, 'κ': 62, 'A': 63, 'λ': 64, 'ὕ': 65, 'ָ': 66, 'n': 67, 'k': 68, 'י': 69, 'T': 70, 'M': 71, 'o': 72, 'ῥ': 73, 'υ': 74, 'x': 75, 'ἰ': 76, 'l': 77, 'Æ': 78, ';': 79, '(': 80, 'F': 81, 'u': 82, 'ό': 83, ']': 84, '\\n': 85, 'ι': 86, 'ו': 87, 'C': 88, '?': 89, 'χ': 90, 'I': 91, ',': 92, 'Y': 93, 'z': 94, '2': 95, 'H': 96, '1': 97, 'ō': 98, 'η': 99, 'B': 100, 'ְ': 101, 'N': 102, 'ύ': 103, 'ί': 104, '0': 105, ')': 106, 'd': 107, 'π': 108, 's': 109, 'ῤ': 110, '7': 111, 'r': 112, 'g': 113, 'S': 114, 'h': 115, 'ח': 116, '’': 117, '—': 118, 'J': 119}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleGPTPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # K, V, Qとかの計算->残渣結合->正規化->FFN (Feed Forward Network)とかをやってるぽい\n",
        "        # Attention計算では、QとKの（各トークンのKにたいする）内積を計算して、それをベクトルとしてもつ。そのベクトルをSoftmaxで重みにする。\n",
        "        # 各トークンのVと重みつき平均を取る。\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embed_size, num_heads),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(embed_size, num_heads),\n",
        "            num_layers=2\n",
        "        )\n",
        "        \n",
        "        # Linearは、行列積 shapeは？\n",
        "        # \n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # ソースをエンコード\n",
        "        src_embedded = self.embedding(src)\n",
        "        encoded = self.encoder(src_embedded)\n",
        "        \n",
        "        # ターゲットをデコード\n",
        "        tgt_embedded = self.embedding(tgt)\n",
        "        \n",
        "        # ★追加3: 因果マスク\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0))\n",
        "        \n",
        "        decoded = self.decoder(tgt_embedded, encoded, tgt_mask=tgt_mask)\n",
        "        output = self.lm_head(decoded)\n",
        "        \n",
        "        return output\n",
        "        \n",
        "    # ★追加4: マスク生成メソッド\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = torch.triu(torch.ones(sz, sz, device=device)) == 1\n",
        "        mask = mask.transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "with open('inputLearnText.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = list(set(text))  #hello->heloみたいにする。\n",
        "char_to_id = {ch: i for i, ch in enumerate(chars)} # 文字: idの配列\n",
        "id_to_char = {i: ch for i, ch in enumerate(chars)} # id: 文字の配列\n",
        "\n",
        "print(\"文字→ID辞書:\", char_to_id)\n",
        "\n",
        "# テキストを1文字ずつIDに変換\n",
        "def text_to_ids(text):\n",
        "    return [char_to_id[ch] for ch in text]\n",
        "\n",
        "# IDを1文字ずつテキストに変換\n",
        "def ids_to_text(ids):\n",
        "    return ''.join([id_to_char[i] for i in ids])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SimpleGPTPredictor(vocab_size=len(chars), embed_size=32, num_heads=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzHLLWKSg3pa"
      },
      "source": [
        "<h2>学習用コード</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_frHUYuSZxSk",
        "outputId": "0551958b-9248-4976-d11b-a0d519c4bd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "学習データ数: 51642\n",
            "例 - 入力: ' Fathers w'\n",
            "例 - 正解: 'Fathers wi'\n",
            "\n",
            "学習開始...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.0031\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 99\u001b[0m\n\u001b[1;32m     70\u001b[0m  \u001b[38;5;66;03m# (batch, seq, vocab) ->  #(seq, )\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# ------- before -------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# train_targetsは(batch*seq)の一次元。要するに次元を減らしている。（数値の変更とかはない）\u001b[39;00m\n\u001b[1;32m     98\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chars)), tgt_batch\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 99\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    100\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    102\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    627\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[1;32m    355\u001b[0m     tensors,\n\u001b[1;32m    356\u001b[0m     grad_tensors_,\n\u001b[1;32m    357\u001b[0m     retain_graph,\n\u001b[1;32m    358\u001b[0m     create_graph,\n\u001b[1;32m    359\u001b[0m     inputs_tuple,\n\u001b[1;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    362\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# テキストを1文字ずつIDに変換\n",
        "def text_to_ids(text):\n",
        "    return [char_to_id[ch] for ch in text]\n",
        "\n",
        "# IDを1文字ずつテキストに変換\n",
        "def ids_to_text(ids):\n",
        "    return ''.join([id_to_char[i] for i in ids])\n",
        "\n",
        "# 簡単な学習データ作成\n",
        "def create_training_data(text, seq_len=10):\n",
        "    ids = text_to_ids(text)\n",
        "    src_data, tgt_data = [], []\n",
        "\n",
        "    for i in range(len(ids) - seq_len):\n",
        "        src_data.append(ids[i:i+seq_len])      # 入力：10文字\n",
        "        tgt_data.append(ids[i+1:i+seq_len+1])  # 正解：1文字ずらした10文字\n",
        "\n",
        "    return torch.tensor(src_data, device=device), torch.tensor(tgt_data, device=device)\n",
        "\n",
        "\n",
        "# 学習データ作成\n",
        "# 追加する\n",
        "with open('inputLearnText.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# これなんだっけ\n",
        "chars = set(text)\n",
        "\n",
        "char_to_id = {ch: i for i, ch in enumerate(chars)} # 文字: idの配列\n",
        "id_to_char = {i: ch for i, ch in enumerate(chars)} # id: 文字の配列\n",
        "\n",
        "train_inputs, train_targets = create_training_data(text)\n",
        "\n",
        "print(f\"学習データ数: {len(train_inputs)}\")\n",
        "print(f\"例 - 入力: '{ids_to_text(train_inputs[20].tolist())}'\")\n",
        "print(f\"例 - 正解: '{ids_to_text(train_targets[20].tolist())}'\")\n",
        "\n",
        "\n",
        "model = SimpleGPTPredictor(vocab_size=len(chars), embed_size=32, num_heads=4)\n",
        "model.to(device)\n",
        "# 学習設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# 損失関数の種類\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\n学習開始...\")\n",
        "# range\n",
        "train_inputs = train_inputs[:5000]\n",
        "train_targets = train_targets[:5000]\n",
        "\n",
        "train_src, train_tgt = create_training_data(text)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    batch_size = 256\n",
        "    \n",
        "    for i in range(0, len(train_src), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        src_batch = train_src[i:i+batch_size]\n",
        "        tgt_batch = train_tgt[i:i+batch_size]\n",
        "        \n",
        "        # ★変更: 2つの引数を渡す\n",
        "        output = model(src_batch, tgt_batch)\n",
        "        \n",
        "         # (batch, seq, vocab) ->  #(seq, )\n",
        "        # \n",
        "        # ------- before -------\n",
        "        # \n",
        "        # \n",
        "        # original_tensor = [\n",
        "        # [  # バッチ1\n",
        "        #     [1, 2, 3, 4],  # 位置1の予測ベクトル\n",
        "        #     [5, 6, 7, 8],  # 位置2の予測ベクトル  \n",
        "        #     [9,10,11,12]   # 位置3の予測ベクトル\n",
        "        # ],\n",
        "        # [  # バッチ2\n",
        "        #     [13,14,15,16], # 位置1の予測ベクトル\n",
        "        #     [17,18,19,20], # 位置2の予測ベクトル\n",
        "        #     [21,22,23,24]  # 位置3の予測ベクトル\n",
        "        # ]\n",
        "        # ]\n",
        "        #\n",
        "        # ------- after -------\n",
        "        #         reshaped_tensor = [\n",
        "        #   [1, 2, 3, 4],   # バッチ1-位置1\n",
        "        #   [5, 6, 7, 8],   # バッチ1-位置2  \n",
        "        #   [9,10,11,12],   # バッチ1-位置3\n",
        "        #   [13,14,15,16],  # バッチ2-位置1\n",
        "        #   [17,18,19,20],  # バッチ2-位置2\n",
        "        #   [21,22,23,24]   # バッチ2-位置3\n",
        "        # ]\n",
        "        # train_targetsは(batch*seq)の一次元。要するに次元を減らしている。（数値の変更とかはない）\n",
        "        loss = criterion(output.view(-1, len(chars)), tgt_batch.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss/len(train_src):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZPiJSE7kcD1"
      },
      "source": [
        "<h2>予測テスト</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgy_HYznkb59",
        "outputId": "590ae9cd-dc3e-41a8-da05-6c1e8ab306eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "入力テキスト:  The god is a\n",
            "回答:  The god is aS7ggugÆGW)vπηrb“7gugu\n"
          ]
        }
      ],
      "source": [
        "def test_prediction(model: SimpleGPTPredictor, input_text):\n",
        "    input_ids = text_to_ids(input_text)\n",
        "    input_tensor = torch.tensor([input_ids])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        last_char_probs = output[0, -1, :]\n",
        "        probs = torch.softmax(last_char_probs, dim=-1)\n",
        "\n",
        "        # 一位のトークンを呼び出す。\n",
        "        top_prob, top_index = torch.topk(probs, 1)\n",
        "        char_id = top_index.item()  # テンソルから数値を取り出し\n",
        "        predicted_char = id_to_char[char_id]  # IDを文字に変換\n",
        "\n",
        "        return predicted_char\n",
        "\n",
        "def generateSeq(model, text, count = 0):\n",
        "    nextSingleToken = test_prediction(model, text)\n",
        "    if(count < 20):\n",
        "        return generateSeq(model, text+nextSingleToken, count+1)\n",
        "    else:\n",
        "        return text+nextSingleToken\n",
        "\n",
        "prompt = \"The god is a\"\n",
        "completion = generateSeq(model, prompt)\n",
        "print(\"入力テキスト: \", prompt)\n",
        "print(\"回答: \", completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5hdog7Xkbo8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki-ziXqGZxqu"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
